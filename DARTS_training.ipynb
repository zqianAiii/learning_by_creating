{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision as tv\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from DARTS_model import *\n",
    "from models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "image_size = 32\n",
    "label_dim = 10\n",
    "G_in_dim = 100\n",
    "G_out_dim = 3\n",
    "D_in_dim = 3\n",
    "D_out_dim = 1\n",
    "num_channels = [512, 256, 128]\n",
    "\n",
    "GAN_lr = 0.0002\n",
    "betas = (0.5, 0.999)\n",
    "batch_size = 16\n",
    "pretrain_epochs = 100\n",
    "num_epochs = 150\n",
    "save_dir = '/model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = tv.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainset, valset = torch.utils.data.random_split(trainset, [3*len(trainset)//5, 2*len(trainset)//5])\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "testset = tv.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = torch.eye(label_dim, device = torch.device('cuda')).view(label_dim, label_dim, 1, 1)\n",
    "fill = torch.zeros([label_dim, label_dim, image_size, image_size], device = torch.device('cuda'))\n",
    "for i in range(label_dim):\n",
    "    fill[i, i, :, :] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator(G_in_dim, label_dim, G_out_dim, num_channels)\n",
    "D = Discriminator(16, 10, 9)\n",
    "clf =  resnet_transfer()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    G, D, clf = G.cuda(), D.cuda(), clf.cuda()\n",
    "    \n",
    "optim_G = optim.Adam(G.parameters(), lr = GAN_lr, betas = betas)\n",
    "optim_D = optim.SGD(D.parameters(), lr = GAN_lr/2, momentum = 0.9, weight_decay = 3e-4)\n",
    "optim_clf = optim.SGD(clf.parameters(), lr = 0.01, momentum = 0.9)\n",
    "optim_arch = optim.Adam(D.arch_parameters(), lr = 3e-4,  betas = (0.5, 0.999), weight_decay = 1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pretrain GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G.train()\n",
    "D.train()\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 125), G_loss: 0.6949044189453125, D_real_loss: 0.7032783813476563, D_fake_loss: 0.7115841064453124\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-6b100f17ad7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mG_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mD_fake_decision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_real_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mG_running_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mG_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0moptim_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    196\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \"\"\"\n\u001b[0;32m--> 198\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         allow_unreachable=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "while epoch < pretrain_epochs:\n",
    "\n",
    "    G_running_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    D_running_real_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    D_running_fake_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    \n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "\n",
    "        mini_batch = images.size()[0]\n",
    "        x_ = images.cuda(non_blocking = True)\n",
    "        \n",
    "        y_real_ = torch.ones(mini_batch, device = torch.device('cuda'))\n",
    "        y_fake_ = torch.zeros(mini_batch, device = torch.device('cuda'))\n",
    "        c_fill_ = fill[labels]\n",
    "        \n",
    "        # Train discriminator\n",
    "        optim_D.zero_grad()\n",
    "        D_real_decision = D(x_, c_fill_).squeeze()\n",
    "        D_real_loss = D.loss(D_real_decision, y_real_)\n",
    "        D_running_real_loss += D_real_loss.detach()\n",
    "\n",
    "        z_ = torch.randn(mini_batch, G_in_dim, device = torch.device('cuda')).view(-1, G_in_dim, 1, 1)\n",
    "        c_ = (torch.rand(mini_batch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = onehot[c_]\n",
    "        gen_image = G(z_, c_onehot_)\n",
    "\n",
    "        c_fill_ = fill[c_]\n",
    "        D_fake_decision = D(gen_image, c_fill_).squeeze()\n",
    "        D_fake_loss = D.loss(D_fake_decision, y_fake_)\n",
    "        D_running_fake_loss += D_fake_loss.detach()\n",
    "        \n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        D_loss.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        # Train generator\n",
    "        z_ = torch.randn(mini_batch, G_in_dim, device = torch.device('cuda')).view(-1, G_in_dim, 1, 1)\n",
    "        c_ = (torch.rand(mini_batch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = onehot[c_]\n",
    "        \n",
    "        optim_G.zero_grad()\n",
    "        optim_arch.zero_grad()\n",
    "        gen_image = G(z_, c_onehot_)\n",
    "\n",
    "        c_fill_ = fill[c_]\n",
    "        D_fake_decision = D(gen_image, c_fill_).squeeze()\n",
    "        G_loss = G.loss(D_fake_decision, y_real_)\n",
    "        G_running_loss += G_loss.detach()\n",
    "        G_loss.backward()\n",
    "        optim_G.step()\n",
    "\n",
    "        if i%125 == 124:\n",
    "            print('({}, {}), G_loss: {}, D_real_loss: {}, D_fake_loss: {}'.format(epoch, i+1, G_running_loss.item()/(i+1), D_running_real_loss.item()/(i+1), D_running_fake_loss.item()/(i+1)))\n",
    "    \n",
    "    model = [G.state_dict(), D.state_dict(), D.arch_parameters()]\n",
    "    optim = [optim_G.state_dict(), optim_D.state_dict()]\n",
    "    torch.save({'model': model, 'optim': optim, 'epoch': epoch}, 'GAN_checkpoint.pth')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.train()\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while epoch < num_epochs:\n",
    "\n",
    "    G_running_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    D_running_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    clf_train_running_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    clf_val_running_loss = torch.zeros((1, 1), device = torch.device('cuda'))\n",
    "    \n",
    "    for i, ((images, labels), (val_images, val_labels)) in enumerate(zip(trainloader, valloader)):\n",
    "\n",
    "        mini_batch = images.size()[0]\n",
    "        x_ = images.cuda(non_blocking = True)\n",
    "        val_images, val_labels = val_images.cuda(non_blocking = True), val_labels.cuda(non_blocking = True)\n",
    "        \n",
    "        y_real_ = torch.ones(mini_batch, device = torch.device('cuda'))\n",
    "        y_fake_ = torch.zeros(mini_batch, device = torch.device('cuda'))\n",
    "        c_fill_ = fill[labels]\n",
    "        \n",
    "        # Train discriminator\n",
    "        optim_D.zero_grad()\n",
    "        D_real_decision = D(x_, c_fill_).squeeze()\n",
    "        D_real_loss = D.loss(D_real_decision, y_real_)\n",
    "\n",
    "        z_ = torch.randn(mini_batch, G_in_dim, device = torch.device('cuda')).view(-1, G_in_dim, 1, 1)\n",
    "        c_ = (torch.rand(mini_batch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = onehot[c_]\n",
    "        gen_image = G(z_, c_onehot_)\n",
    "\n",
    "        c_fill_ = fill[c_]\n",
    "        D_fake_decision = D(gen_image, c_fill_).squeeze()\n",
    "        D_fake_loss = D.loss(D_fake_decision, y_fake_)\n",
    "        \n",
    "        D_loss = D_real_loss + D_fake_loss\n",
    "        D_running_loss += D_loss.detach()\n",
    "        D_loss.backward()\n",
    "        optim_D.step()\n",
    "        \n",
    "        # Train generator\n",
    "        z_ = torch.randn(mini_batch, G_in_dim, device = torch.device('cuda')).view(-1, G_in_dim, 1, 1)\n",
    "        c_ = (torch.rand(mini_batch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = onehot[c_]\n",
    "        \n",
    "        optim_G.zero_grad()\n",
    "        optim_arch.zero_grad()\n",
    "        gen_image = G(z_, c_onehot_)\n",
    "\n",
    "        c_fill_ = fill[c_]\n",
    "        D_fake_decision = D(gen_image, c_fill_).squeeze()\n",
    "        G_loss = G.loss(D_fake_decision, y_real_)\n",
    "        G_running_loss += G_loss.detach()\n",
    "        G_loss.backward(create_graph = True)\n",
    "        optim_G.step()\n",
    "        \n",
    "        # Train Resnet\n",
    "        z_ = torch.randn(mini_batch, G_in_dim, device = torch.device('cuda')).view(-1, G_in_dim, 1, 1)\n",
    "        c_ = (torch.rand(mini_batch, 1) * label_dim).type(torch.LongTensor).squeeze()\n",
    "        c_onehot_ = onehot[c_]\n",
    "        \n",
    "        c_ = c_.cuda(non_blocking = True)\n",
    "        labels = labels.cuda(non_blocking = True)\n",
    "        \n",
    "        gen_image = G(z_, c_onehot_)\n",
    "        \n",
    "        optim_clf.zero_grad()\n",
    "        clf_fake_decision = clf(gen_image)\n",
    "        clf_fake_loss = clf.loss(clf_fake_decision, c_)      \n",
    "        clf_real_decision = clf(x_)\n",
    "        clf_real_loss = clf.loss(clf_real_decision, labels)\n",
    "        \n",
    "        clf_loss = clf_fake_loss + clf_real_loss\n",
    "        clf_train_running_loss += clf_real_loss.detach()\n",
    "        clf_loss.backward(create_graph = True)\n",
    "        optim_clf.step()\n",
    "        \n",
    "        # Train architecture\n",
    "        y = clf(val_images)\n",
    "        loss = clf.loss(y, val_labels)\n",
    "        clf_val_running_loss += loss.detach()\n",
    "        loss.backward()\n",
    "        optim_arch.step()\n",
    "\n",
    "        for param in G.parameters():\n",
    "            param.grad = None\n",
    "        for param in D.parameters():\n",
    "            param.grad = None\n",
    "        for param in clf.parameters():\n",
    "            param.grad = None\n",
    "        for param in D.arch_parameters():\n",
    "            param.grad = None\n",
    "\n",
    "        if i%125 == 124:\n",
    "            print('({}, {}), G_loss: {}, D_loss: {}, clf_train: {}, clf_val: {}'.format(epoch, i+1, G_running_loss.item()/(i+1), D_running_loss.item()/(i+1), clf_train_running_loss.item()/(i+1), clf_val_running_loss.item()/(i+1)))\n",
    "            print(D.alphas_normal[0])\n",
    "\n",
    "    model = [G.state_dict(), D.state_dict(), clf.state_dict(), D.arch_parameters()]\n",
    "    optim = [optim_G.state_dict(), optim_D.state_dict(), optim_clf.state_dict(), optim_arch.state_dict()]\n",
    "    torch.save({'model': model, 'optim': optim, 'epoch': epoch}, 'checkpoint.pth')\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
